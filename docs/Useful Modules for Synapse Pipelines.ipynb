{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Modules for Synapse Pipelines\n",
    "In running the service and making sure everything works with AWS and is integrated with the NeuroData stack, we were required to write a lot of boiler-plate code to interface with these different tools. We will cover these modules in this notebook. Look at the example pipeline (nomads_unsupervised) to see the different modules we provide.\n",
    "\n",
    "The following describes the various modules:\n",
    "1. nd_boss.py - For pushing results to BOSS\n",
    "2. NeuroDataResource.py - For pulling data from BOSS\n",
    "3. driver.py - Integrating the pipeline with AWS\n",
    "4. pymeda_driver.py - Integrating pipeline with PyMeda\n",
    "5. nomads.py - the actual algorithm (we won't go over this here)\n",
    "\n",
    "## nd_boss\n",
    "\n",
    "The main function in this module is boss_push which pushes a numpy array to the BOSS. To run this function, you need to provide:\n",
    "1. token - BOSS API Token (needs permissions to push to col and exp)\n",
    "2. col - the collection you are pushing to\n",
    "3. exp - the experiment you are pushing to\n",
    "4. z_range, y_range, x_range - location of cutout that you are pushing to (better we same size as actual cutout)\n",
    "5. data_dict - dictionary where each key is a channel name and value is numpy array to be pushed\n",
    "6. results_key - idenitfier that contains metadata on the results you are pushing\n",
    "\n",
    "Note that the channel name that is ultimately pushed to BOSS is results_key + key of dictionary. That way metadata and different results from the pipeline (i.e. Gaba detections numpy array, Glut detections numpy array) can all be pushed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boss_push(token,\n",
    "              col,\n",
    "              exp,\n",
    "              z_range,\n",
    "              y_range,\n",
    "              x_range,\n",
    "              data_dict,\n",
    "              results_key):\n",
    "    dtype = \"uint8\"\n",
    "    config_dict = {\"token\": token, \"host\": \"api.boss.neurodata.io\" , \"protocol\": \"https\"}\n",
    "    remote = create_boss_remote(config_dict)\n",
    "    links_dict = {}\n",
    "\n",
    "    for key, data in data_dict.items():\n",
    "        data = data.astype(np.uint8)\n",
    "        np.putmask(data, data>0, 255)\n",
    "        channel = results_key + \"_\" + key\n",
    "        print(data.shape)\n",
    "        z, y, x = data.shape\n",
    "\n",
    "        channel_resource = ChannelResource(channel, col, exp, 'image', '', 0, dtype, 0)\n",
    "        print(\"Pushing to BOSS...\")\n",
    "\n",
    "        for z in range(z_range[0],z_range[1]):\n",
    "            print(z)\n",
    "            try:\n",
    "                old_channel = remote.get_project(channel_resource)\n",
    "                remote.create_cutout(old_channel, 0, (x_range[0],x_range[1]), (y_range[0],y_range[1]), (z,z+1), data[z-z_range[0]].reshape(-1,data[z-z_range[0]].shape[0],data[z-z_range[0]].shape[1]))\n",
    "            except:\n",
    "                channel_resource = ChannelResource(channel, col, exp, 'image', '', 0, dtype, 0)#, sources = [\"em_clahe\"])\n",
    "                new_channel = remote.create_project(channel_resource)\n",
    "                remote.create_cutout(new_channel, 0, (x_range[0],x_range[1]), (y_range[0],y_range[1]), (z,z+1), data[z-z_range[0]].reshape(-1,data[z-z_range[0]].shape[0],data[z-z_range[0]].shape[1]))\n",
    "\n",
    "\n",
    "        links_dict[\"All Predictions\"] = (\"http://ndwt.neurodata.io/channel_detail/{}/{}/{}/\").format(col, exp, channel)\n",
    "        print(\"Pushed {} to Boss\".format(channel))\n",
    "    return links_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuroDataResource\n",
    "This is just a lightweight intern wrapper. We won't go over the code here since there shouldln't be a need to change it for the most part.\n",
    "\n",
    "## driver\n",
    "This is the main file that is used when AWS Batch launches the job! The majority of the file are just a series of functions that nomads_unsupervised uses to run properly. We will not go over these here since they are pipline specific. Below are more useful ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull data from BOSS\n",
    "def get_data(host, token, col, exp, z_range, y_range, x_range):\n",
    "    print(\"Downloading {} from {} with ranges: z: {} y: {} x: {}\".format(exp,\n",
    "                                                                         col,\n",
    "                                                                         str(z_range),\n",
    "                                                                         str(y_range),\n",
    "                                                                         str(x_range)))\n",
    "    resource = NeuroDataResource(host, token, col, exp)\n",
    "    data_dict = {}\n",
    "    for chan in resource.channels:\n",
    "        data_dict[chan] = resource.get_cutout(chan, z_range, y_range, x_range)\n",
    "    return data_dict, resource.voxel_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This get_data function just wraps NeuroDataResource to grab a data dictionary where each key is channel and each value is numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_results(path, results_key):\n",
    "    client = boto3.client('s3')\n",
    "    s3 = boto3.resource('s3')\n",
    "    s3_bucket_exists_waiter = client.get_waiter('bucket_exists')\n",
    "    bucket = client.create_bucket(Bucket=\"nomads-unsupervised-results\")\n",
    "    s3_bucket_exists_waiter.wait(Bucket=\"nomads-unsupervised-results\")\n",
    "\n",
    "    bucket = s3.Bucket(\"nomads-unsupervised-results\")\n",
    "    bucket.Acl().put(ACL='public-read')\n",
    "    files = glob.glob(path+\"*\")\n",
    "    for file in files:\n",
    "        key = results_key + \"/\" + file.split(\"/\")[-1]\n",
    "        client.upload_file(file, \"nomads-unsupervised-results\", key)\n",
    "        response = client.put_object_acl(ACL='public-read', Bucket=\"nomads-unsupervised-results\", \\\n",
    "        Key=key)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function pushes results to S3. Note how it does this: it expects all the pipeline outputs (prediction files, PyMeda html) to be pushed to a path. This function then just grabs all the files in path, and pushes them to the proper S3 folder.\n",
    "\n",
    "The S3 folder is labeled with results_key which is used in both the service and in naming channels pushed to BOSS through nd_boss (the results_key parameter in boss_push). This key is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_key = \"_\".join([\"nomads-unsupervised\", col, exp, \"z\", str(z_range[0]), str(z_range[1]), \"y\", \\\n",
    "    str(y_range[0]), str(y_range[1]), \"x\", str(x_range[0]), str(x_range[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one really important thing to note about driver.py. For now, we are running only one command in AWS Batch to make our lives easier. This means only one function can be called! We accomplish this by calling \"python3 driver.py\" in our actual job. This triggers the main function which runs the following as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='NOMADS and PyMeda driver.')\n",
    "    parser.add_argument('--host', required = True, type=str, help='BOSS Api host, do not include \"https\"')\n",
    "    parser.add_argument('--token', required = True, type=str, help='BOSS API Token Key')\n",
    "    parser.add_argument('--col', required = True, type=str, help='collection name')\n",
    "    parser.add_argument('--exp', required = True, type=str, help='experiment name')\n",
    "    parser.add_argument('--z-range', required = True, type=str, help='zstart,zstop   NO SPACES. zstart, zstop will be casted to ints')\n",
    "    parser.add_argument('--y-range', required = True, type=str, help='ystart,ystop   NO SPACES. ystart, ystop will be casted to ints')\n",
    "    parser.add_argument('--x-range', required = True, type=str, help='xstart,xstop   NO SPACES. xstart, xstop will be casted to ints')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    z_range = list(map(int, args.z_range.split(\",\")))\n",
    "    y_range = list(map(int, args.y_range.split(\",\")))\n",
    "    x_range = list(map(int, args.x_range.split(\",\")))\n",
    "\n",
    "    driver(args.host, args.token, args.col, args.exp, z_range, y_range, x_range)\n",
    "    \n",
    "\n",
    "def driver(host, token, col, exp, z_range, y_range, x_range, path = \"./results/\"):\n",
    "\n",
    "    print(\"Starting Nomads Unsupervised...\")\n",
    "    info = locals()\n",
    "    data_dict, voxel_size = get_data(host, token, col, exp, z_range, y_range, x_range)\n",
    "\n",
    "    results = run_nomads(data_dict)\n",
    "    results = results.astype(np.uint8)\n",
    "    np.putmask(results, results, 255)\n",
    "\n",
    "    results_key = \"_\".join([\"nomads-unsupervised\", col, exp, \"z\", str(z_range[0]), str(z_range[1]), \"y\", \\\n",
    "    str(y_range[0]), str(y_range[1]), \"x\", str(x_range[0]), str(x_range[1])])\n",
    "\n",
    "    pickle.dump(results, open(path + \"nomads-unsupervised-predictions\" + \".pkl\", \"wb\"))\n",
    "    print(\"Saved pickled results (np array) {} in {}\".format(\"nomads-unsupervised-predictions.pkl\", path))\n",
    "\n",
    "    print(\"Generating PyMeda Plots...\")\n",
    "\n",
    "    norm_data = load_and_preproc(data_dict)\n",
    "    try:\n",
    "        pymeda_driver.pymeda_pipeline(results, norm_data, title = \"PyMeda Plots on All Predicted Synapses\", path = path)\n",
    "    except:\n",
    "        print(\"Not generating plots for all synapses, no predictions classified as Gaba\")\n",
    "    print(\"Uploading results...\")\n",
    "    #results = pickle.load(open(\"./results/nomads-unsupervised-predictions.pkl\", \"rb\"))\n",
    "\n",
    "    boss_links = boss_push(token, \"collman_nomads\", \"nomads_predictions\", z_range, y_range, x_range, {results_key: results}, results_key)\n",
    "    with open('results/NDVIS_links.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in boss_links.items():\n",
    "            writer.writerow([key, value])\n",
    "    upload_results(path, results_key)\n",
    "\n",
    "    return info, results, boss_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: Note that there seems to be a lot going on. But all driver() function does is incorporate all the different steps that have to happen in the pipeline (pull data from BOSS, run algorithm, run PyMeda, push to BOSS, push to S3). When adding new pipelines, you should follow a similar format and just call \"driver.py\" during job submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
